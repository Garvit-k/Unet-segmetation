{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tqdm import tqdm\nimport os\nimport time\nfrom random import randint\n\nimport numpy as np\nfrom scipy import stats\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import KFold\n\nimport nibabel as nib\nimport pydicom as pdm\nimport nilearn as nl\nimport nilearn.plotting as nlplt\nimport h5py\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport matplotlib.animation as anim\nimport matplotlib.patches as mpatches\nimport matplotlib.gridspec as gridspec\n\nimport seaborn as sns\nimport imageio\nfrom skimage.transform import resize\nfrom skimage.util import montage\n\nfrom IPython.display import Image as show_gif\nfrom IPython.display import clear_output\nfrom IPython.display import YouTubeVideo\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn import MSELoss\n\n\nimport albumentations as A\nfrom albumentations import Compose, HorizontalFlip\nfrom albumentations.pytorch import ToTensor, ToTensorV2 \n\nimport shutil\n\nimport warnings\nwarnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:43:58.070713Z","iopub.execute_input":"2021-05-30T17:43:58.071099Z","iopub.status.idle":"2021-05-30T17:44:01.872988Z","shell.execute_reply.started":"2021-05-30T17:43:58.071053Z","shell.execute_reply":"2021-05-30T17:44:01.872194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_filename = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_flair.nii'\nsample_filename_mask = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_seg.nii'\n\nsample_img = nib.load(sample_filename)\nsample_img = np.asanyarray(sample_img.dataobj)\nsample_img = np.rot90(sample_img)\nsample_mask = nib.load(sample_filename_mask)\nsample_mask = np.asanyarray(sample_mask.dataobj)\nsample_mask = np.rot90(sample_mask)\nprint(\"img shape ->\", sample_img.shape)\nprint(\"mask shape ->\", sample_mask.shape)\n\n\nsample_filename2 = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_t1.nii'\nsample_img2 = nib.load(sample_filename2)\nsample_img2 = np.asanyarray(sample_img2.dataobj)\nsample_img2  = np.rot90(sample_img2)\n\nsample_filename3 = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_t2.nii'\nsample_img3 = nib.load(sample_filename3)\nsample_img3 = np.asanyarray(sample_img3.dataobj)\nsample_img3  = np.rot90(sample_img3)\n\nsample_filename4 = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_t1ce.nii'\nsample_img4 = nib.load(sample_filename4)\nsample_img4 = np.asanyarray(sample_img4.dataobj)\nsample_img4  = np.rot90(sample_img4)\n\nmask_WT = sample_mask.copy()\nmask_WT[mask_WT == 1] = 1\nmask_WT[mask_WT == 2] = 1\nmask_WT[mask_WT == 4] = 1\n\nmask_TC = sample_mask.copy()\nmask_TC[mask_TC == 1] = 1\nmask_TC[mask_TC == 2] = 0\nmask_TC[mask_TC == 4] = 1\n\nmask_ET = sample_mask.copy()\nmask_ET[mask_ET == 1] = 0\nmask_ET[mask_ET == 2] = 0\nmask_ET[mask_ET == 4] = 1\n\n### What's the data looks like ?\n\nfig = plt.figure(figsize=(20, 10))\n\ngs = gridspec.GridSpec(nrows=2, ncols=4, height_ratios=[1, 1.5])\n\n#  Varying density along a streamline\nax0 = fig.add_subplot(gs[0, 0])\nflair = ax0.imshow(sample_img[:,:,65], cmap='bone')\nax0.set_title(\"FLAIR\", fontsize=18, weight='bold', y=-0.2)\nfig.colorbar(flair)\n\n#  Varying density along a streamline\nax1 = fig.add_subplot(gs[0, 1])\nt1 = ax1.imshow(sample_img2[:,:,65], cmap='bone')\nax1.set_title(\"T1\", fontsize=18, weight='bold', y=-0.2)\nfig.colorbar(t1)\n\n#  Varying density along a streamline\nax2 = fig.add_subplot(gs[0, 2])\nt2 = ax2.imshow(sample_img3[:,:,65], cmap='bone')\nax2.set_title(\"T2\", fontsize=18, weight='bold', y=-0.2)\nfig.colorbar(t2)\n\n#  Varying density along a streamline\nax3 = fig.add_subplot(gs[0, 3])\nt1ce = ax3.imshow(sample_img4[:,:,65], cmap='bone')\nax3.set_title(\"T1 contrast\", fontsize=18, weight='bold', y=-0.2)\nfig.colorbar(t1ce)\n\n#  Varying density along a streamline\nax4 = fig.add_subplot(gs[1, 1:3])\n\n#ax4.imshow(np.ma.masked_where(mask_WT[:,:,65]== False,  mask_WT[:,:,65]), cmap='summer', alpha=0.6)\nl1 = ax4.imshow(mask_WT[:,:,65], cmap='summer',)\nl2 = ax4.imshow(np.ma.masked_where(mask_TC[:,:,65]== False,  mask_TC[:,:,65]), cmap='rainbow', alpha=0.6)\nl3 = ax4.imshow(np.ma.masked_where(mask_ET[:,:,65] == False, mask_ET[:,:,65]), cmap='winter', alpha=0.6)\n\nax4.set_title(\"\", fontsize=20, weight='bold', y=-0.1)\n\n_ = [ax.set_axis_off() for ax in [ax0,ax1,ax2,ax3, ax4]]\n\ncolors = [im.cmap(im.norm(1)) for im in [l1,l2, l3]]\nlabels = ['Non-Enhancing tumor core', 'Peritumoral Edema ', 'GD-enhancing tumor']\npatches = [ mpatches.Patch(color=colors[i], label=f\"{labels[i]}\") for i in range(len(labels))]\n# put those patched as legend-handles into the legend\nplt.legend(handles=patches, bbox_to_anchor=(1.1, 0.65), loc=2, borderaxespad=0.4,fontsize = 'xx-large',\n           title='Mask Labels', title_fontsize=18, edgecolor=\"black\",  facecolor='#c5c6c7')\n\nplt.suptitle(\"Multimodal Scans -  Data | Manually-segmented mask - Target\", fontsize=20, weight='bold')\n\nfig.savefig(\"data_sample.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\nfig.savefig(\"data_sample.svg\", format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:01.874748Z","iopub.execute_input":"2021-05-30T17:44:01.875038Z","iopub.status.idle":"2021-05-30T17:44:03.696093Z","shell.execute_reply.started":"2021-05-30T17:44:01.875012Z","shell.execute_reply":"2021-05-30T17:44:03.695221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"helper classes and functions","metadata":{}},{"cell_type":"code","source":"class Image3dToGIF3d:\n    \n    def __init__(self, \n                 img_dim: tuple = (55, 55, 55),\n                 figsize: tuple = (15, 10),\n                 binary: bool = False,\n                 normalizing: bool = True,\n                ):\n        \"\"\"Initialization.\"\"\"\n        self.img_dim = img_dim\n        print(img_dim)\n        self.figsize = figsize\n        self.binary = binary\n        self.normalizing = normalizing\n\n    def _explode(self, data: np.ndarray):\n        \"\"\"\n        Takes: array and return an array twice as large in each dimension,\n        \"\"\"\n        shape_arr = np.array(data.shape)\n        size = shape_arr[:3] * 2 - 1\n        exploded = np.zeros(np.concatenate([size, shape_arr[3:]]),\n                            dtype=data.dtype)\n        exploded[::2, ::2, ::2] = data\n        return exploded\n\n    def _expand_coordinates(self, indices: np.ndarray):\n        x, y, z = indices\n        x[1::2, :, :] += 1\n        y[:, 1::2, :] += 1\n        z[:, :, 1::2] += 1\n        return x, y, z\n    \n    def _normalize(self, arr: np.ndarray):\n        \"\"\"Normilize image value between 0 and 1.\"\"\"\n        arr_min = np.min(arr)\n        return (arr - arr_min) / (np.max(arr) - arr_min)\n\n    \n    def _scale_by(self, arr: np.ndarray, factor: int):\n        \n        mean = np.mean(arr)\n        return (arr - mean) * factor + mean\n    \n    def get_transformed_data(self, data: np.ndarray):\n        \"\"\"Data transformation: normalization, scaling, resizing.\"\"\"\n        if self.binary:\n            resized_data = resize(data, self.img_dim, preserve_range=True)\n            return np.clip(resized_data.astype(np.uint8), 0, 1).astype(np.float32)\n            \n        norm_data = np.clip(self._normalize(data)-0.1, 0, 1) ** 0.4\n        scaled_data = np.clip(self._scale_by(norm_data, 2) - 0.1, 0, 1)\n        resized_data = resize(scaled_data, self.img_dim, preserve_range=True)\n        \n        return resized_data\n    \n    def plot_cube(self,\n                  cube,\n                  title: str = '', \n                  init_angle: int = 0,\n                  make_gif: bool = False,\n                  path_to_save: str = 'filename.gif'\n                 ):\n        \n        if self.binary:\n            facecolors = cm.winter(cube)\n            print(\"binary\")\n        else:\n            if self.normalizing:\n                cube = self._normalize(cube)\n            facecolors = cm.gist_stern(cube)\n            print(\"not binary\")\n            \n        facecolors[:,:,:,-1] = cube\n        facecolors = self._explode(facecolors)\n\n        filled = facecolors[:,:,:,-1] != 0\n        x, y, z = self._expand_coordinates(np.indices(np.array(filled.shape) + 1))\n\n        with plt.style.context(\"dark_background\"):\n\n            fig = plt.figure(figsize=self.figsize)\n            ax = fig.gca(projection='3d')\n\n            ax.view_init(30, init_angle)\n            ax.set_xlim(right = self.img_dim[0] * 2)\n            ax.set_ylim(top = self.img_dim[1] * 2)\n            ax.set_zlim(top = self.img_dim[2] * 2)\n            ax.set_title(title, fontsize=18, y=1.05)\n\n            ax.voxels(x, y, z, filled, facecolors=facecolors, shade=False)\n\n            if make_gif:\n                images = []\n                for angle in tqdm(range(0, 360, 5)):\n                    ax.view_init(30, angle)\n                    fname = str(angle) + '.png'\n\n                    plt.savefig(fname, dpi=120, format='png', bbox_inches='tight')\n                    images.append(imageio.imread(fname))\n                    #os.remove(fname)\n                imageio.mimsave(path_to_save, images)\n                plt.close()\n\n            else:\n                plt.show()\n\n                \nclass ShowResult:\n  \n    def mask_preprocessing(self, mask):\n        \"\"\"\n        Test.\n        \"\"\"\n        mask = mask.squeeze().cpu().detach().numpy()\n        mask = np.moveaxis(mask, (0, 1, 2, 3), (0, 3, 2, 1))\n\n        mask_WT = np.rot90(montage(mask[0]))\n        mask_TC = np.rot90(montage(mask[1]))\n        mask_ET = np.rot90(montage(mask[2]))\n\n        return mask_WT, mask_TC, mask_ET\n\n    def image_preprocessing(self, image):\n        \n        image = image.squeeze().cpu().detach().numpy()\n        image = np.moveaxis(image, (0, 1, 2, 3), (0, 3, 2, 1))\n        flair_img = np.rot90(montage(image[0]))\n        return flair_img\n    \n    def plot(self, image, ground_truth, prediction):\n        image = self.image_preprocessing(image)\n        gt_mask_WT, gt_mask_TC, gt_mask_ET = self.mask_preprocessing(ground_truth)\n        pr_mask_WT, pr_mask_TC, pr_mask_ET = self.mask_preprocessing(prediction)\n        \n        fig, axes = plt.subplots(1, 2, figsize = (35, 30))\n    \n        [ax.axis(\"off\") for ax in axes]\n        axes[0].set_title(\"Ground Truth\", fontsize=35, weight='bold')\n        axes[0].imshow(image, cmap ='bone')\n        axes[0].imshow(np.ma.masked_where(gt_mask_WT == False, gt_mask_WT),\n                  cmap='cool_r', alpha=0.6)\n        axes[0].imshow(np.ma.masked_where(gt_mask_TC == False, gt_mask_TC),\n                  cmap='autumn_r', alpha=0.6)\n        axes[0].imshow(np.ma.masked_where(gt_mask_ET == False, gt_mask_ET),\n                  cmap='autumn', alpha=0.6)\n\n        axes[1].set_title(\"Prediction\", fontsize=35, weight='bold')\n        axes[1].imshow(image, cmap ='bone')\n        axes[1].imshow(np.ma.masked_where(pr_mask_WT == False, pr_mask_WT),\n                  cmap='cool_r', alpha=0.6)\n        axes[1].imshow(np.ma.masked_where(pr_mask_TC == False, pr_mask_TC),\n                  cmap='autumn_r', alpha=0.6)\n        axes[1].imshow(np.ma.masked_where(pr_mask_ET == False, pr_mask_ET),\n                  cmap='autumn', alpha=0.6)\n\n        plt.tight_layout()\n        \n        plt.show()\n        \n#show_result = ShowResult()\n#show_result.plot(data['image'], data['mask'], data['mask'])\n\n\ndef merging_two_gif(path1: str, path2: str, name_to_save: str):\n    \"\"\"\n    Merging GIFs side by side.\n    \"\"\"\n    \n    gif1 = imageio.get_reader(path1)\n    gif2 = imageio.get_reader(path2)\n\n    \n    number_of_frames = min(gif1.get_length(), gif2.get_length()) \n\n    #Create writer object\n    new_gif = imageio.get_writer(name_to_save)\n\n    for frame_number in range(number_of_frames):\n        img1 = gif1.get_next_data()\n        img2 = gif2.get_next_data()\n        \n        new_image = np.hstack((img1, img2))\n        new_gif.append_data(new_image)\n\n    gif1.close()\n    gif2.close()    \n    new_gif.close()\n    \n#merging_two_gif('BraTS20_Training_001_flair_3d.gif',\n#                'BraTS20_Training_001_flair_3d.gif', \n#                'result.gif')\n\ndef get_all_csv_file(root: str) -> list:\n    \"\"\"Extraction all unique ids from file names.\"\"\"\n    ids = []\n    for dirname, _, filenames in os.walk(root):\n        for filename in filenames:\n            path = os.path.join(dirname, filename)\n            if path.endswith(\".csv\"):\n                ids.append(path) \n    ids = list(set(filter(None, ids)))\n    print(f\"Extracted {len(ids)} csv files.\")\n    return ids\n\n#csv_paths = get_all_csv_file(\"../input/brats20-dataset-training-validation/BraTS2020_TrainingData\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-30T17:44:03.697408Z","iopub.execute_input":"2021-05-30T17:44:03.697754Z","iopub.status.idle":"2021-05-30T17:44:03.750488Z","shell.execute_reply.started":"2021-05-30T17:44:03.697721Z","shell.execute_reply":"2021-05-30T17:44:03.749547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GlobalConfig:\n    root_dir = '../input/brats20-dataset-training-validation'\n    train_root_dir = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n    test_root_dir = '../input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'\n    path_to_csv = './train_data.csv'\n    pretrained_model_path = '../input/brats20logs/brats2020logs/unet/last_epoch_model.pth'\n    train_logs_path = '../input/brats20logs/brats2020logs/unet/train_log.csv'\n    ae_pretrained_model_path = '../input/brats20logs/brats2020logs/ae/autoencoder_best_model.pth'\n    tab_data = '../input/brats20logs/brats2020logs/data/df_with_voxel_stats_and_latent_features.csv'\n    seed = 55\n    \ndef seed_everything(seed: int):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    \nconfig = GlobalConfig()\nseed_everything(config.seed)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:03.751948Z","iopub.execute_input":"2021-05-30T17:44:03.752406Z","iopub.status.idle":"2021-05-30T17:44:03.850459Z","shell.execute_reply.started":"2021-05-30T17:44:03.752364Z","shell.execute_reply":"2021-05-30T17:44:03.849723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Preprocessing","metadata":{}},{"cell_type":"code","source":"survival_info_df = pd.read_csv('../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/survival_info.csv')\nname_mapping_df = pd.read_csv('../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/name_mapping.csv')\n\nname_mapping_df.rename({'BraTS_2020_subject_ID': 'Brats20ID'}, axis=1, inplace=True) \n\n\ndf = survival_info_df.merge(name_mapping_df, on=\"Brats20ID\", how=\"right\")\n\npaths = []\nfor _, row  in df.iterrows():\n    \n    id_ = row['Brats20ID']\n    phase = id_.split(\"_\")[-2]\n    \n    if phase == 'Training':\n        path = os.path.join(config.train_root_dir, id_)\n    else:\n        path = os.path.join(config.test_root_dir, id_)\n    paths.append(path)\n    \ndf['path'] = paths\n\n#split data on train, test, split\n#train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=69, shuffle=True)\n#train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n\ntrain_data = df.loc[df['Age'].notnull()].reset_index(drop=True)\ntrain_data[\"Age_rank\"] =  train_data[\"Age\"] // 10 * 10\ntrain_data = train_data.loc[train_data['Brats20ID'] != 'BraTS20_Training_355'].reset_index(drop=True, )\n\nskf = StratifiedKFold(\n    n_splits=7, random_state=config.seed, shuffle=True\n)\nfor i, (train_index, val_index) in enumerate(\n        skf.split(train_data, train_data[\"Age_rank\"])\n        ):\n        train_data.loc[val_index, \"fold\"] = i\n\ntrain_df = train_data.loc[train_data['fold'] != 0].reset_index(drop=True)\nval_df = train_data.loc[train_data['fold'] == 0].reset_index(drop=True)\n\ntest_df = df.loc[~df['Age'].notnull()].reset_index(drop=True)\nprint(\"train_df ->\", train_df.shape, \"val_df ->\", val_df.shape, \"test_df ->\", test_df.shape)\ntrain_data.to_csv(\"train_data.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:03.854011Z","iopub.execute_input":"2021-05-30T17:44:03.854283Z","iopub.status.idle":"2021-05-30T17:44:04.354377Z","shell.execute_reply.started":"2021-05-30T17:44:03.854256Z","shell.execute_reply":"2021-05-30T17:44:04.353625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset DataLoader","metadata":{}},{"cell_type":"code","source":"class BratsDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, phase: str=\"test\", is_resize: bool=False):\n        self.df = df\n        self.phase = phase\n        self.augmentations = get_augmentations(phase)\n        self.data_types = ['_flair.nii', '_t1.nii', '_t1ce.nii', '_t2.nii']\n        self.is_resize = is_resize\n        \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, idx):\n        id_ = self.df.loc[idx, 'Brats20ID']\n        root_path = self.df.loc[self.df['Brats20ID'] == id_]['path'].values[0]\n        # load all modalities\n        images = []\n        for data_type in self.data_types:\n            img_path = os.path.join(root_path, id_ + data_type)\n            img = self.load_img(img_path)#.transpose(2, 0, 1)\n            \n            if self.is_resize:\n                img = self.resize(img)\n    \n            img = self.normalize(img)\n            images.append(img)\n        img = np.stack(images)\n        img = np.moveaxis(img, (0, 1, 2, 3), (0, 3, 2, 1))\n        \n        if self.phase != \"test\":\n            mask_path =  os.path.join(root_path, id_ + \"_seg.nii\")\n            mask = self.load_img(mask_path)\n            \n            if self.is_resize:\n                mask = self.resize(mask)\n                mask = np.clip(mask.astype(np.uint8), 0, 1).astype(np.float32)\n                mask = np.clip(mask, 0, 1)\n            mask = self.preprocess_mask_labels(mask)\n    \n            augmented = self.augmentations(image=img.astype(np.float32), \n                                           mask=mask.astype(np.float32))\n            \n            img = augmented['image']\n            mask = augmented['mask']\n    \n        \n            return {\n                \"Id\": id_,\n                \"image\": img,\n                \"mask\": mask,\n            }\n        \n        return {\n            \"Id\": id_,\n            \"image\": img,\n        }\n    \n    def load_img(self, file_path):\n        data = nib.load(file_path)\n        data = np.asarray(data.dataobj)\n        return data\n    \n    def normalize(self, data: np.ndarray):\n        data_min = np.min(data)\n        return (data - data_min) / (np.max(data) - data_min)\n    \n    def resize(self, data: np.ndarray):\n        data = resize(data, (78, 120, 120), preserve_range=True)\n        return data\n    \n    def preprocess_mask_labels(self, mask: np.ndarray):\n\n        mask_WT = mask.copy()\n        mask_WT[mask_WT == 1] = 1\n        mask_WT[mask_WT == 2] = 1\n        mask_WT[mask_WT == 4] = 1\n\n        mask_TC = mask.copy()\n        mask_TC[mask_TC == 1] = 1\n        mask_TC[mask_TC == 2] = 0\n        mask_TC[mask_TC == 4] = 1\n\n        mask_ET = mask.copy()\n        mask_ET[mask_ET == 1] = 0\n        mask_ET[mask_ET == 2] = 0\n        mask_ET[mask_ET == 4] = 1\n\n        mask = np.stack([mask_WT, mask_TC, mask_ET])\n        mask = np.moveaxis(mask, (0, 1, 2, 3), (0, 3, 2, 1))\n\n        return mask\n\n        ","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:04.563733Z","iopub.execute_input":"2021-05-30T17:44:04.564014Z","iopub.status.idle":"2021-05-30T17:44:04.58783Z","shell.execute_reply.started":"2021-05-30T17:44:04.563987Z","shell.execute_reply":"2021-05-30T17:44:04.586966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_augmentations(phase):\n    list_transforms = []\n    \n    list_trfms = Compose(list_transforms)\n    return list_trfms\n\n\ndef get_dataloader(\n    dataset: torch.utils.data.Dataset,\n    path_to_csv: str,\n    phase: str,\n    fold: int = 0,\n    batch_size: int = 1,\n    num_workers: int = 4,\n):\n    '''Returns: dataloader for the model training'''\n    df = pd.read_csv(path_to_csv)\n    \n    train_df = df.loc[df['fold'] != fold].reset_index(drop=True)\n    val_df = df.loc[df['fold'] == fold].reset_index(drop=True)\n\n    df = train_df if phase == \"train\" else val_df\n    dataset = dataset(df, phase)\n    dataloader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=True,\n        shuffle=True,   \n    )\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:05.730998Z","iopub.execute_input":"2021-05-30T17:44:05.731337Z","iopub.status.idle":"2021-05-30T17:44:05.741204Z","shell.execute_reply.started":"2021-05-30T17:44:05.731307Z","shell.execute_reply":"2021-05-30T17:44:05.74007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = get_dataloader(dataset=BratsDataset, path_to_csv='train_data.csv', phase='valid', fold=0)\nlen(dataloader)\ndata = next(iter(dataloader))\ndata['Id'], data['image'].shape, data['mask'].shape","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:06.69557Z","iopub.execute_input":"2021-05-30T17:44:06.695902Z","iopub.status.idle":"2021-05-30T17:44:17.724385Z","shell.execute_reply.started":"2021-05-30T17:44:06.695872Z","shell.execute_reply":"2021-05-30T17:44:17.723582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_tensor = data['image'].squeeze()[0].cpu().detach().numpy() \nmask_tensor = data['mask'].squeeze()[0].squeeze().cpu().detach().numpy()\nprint(\"Num uniq Image values :\", len(np.unique(img_tensor, return_counts=True)[0]))\nprint(\"Min/Max Image values:\", img_tensor.min(), img_tensor.max())\nprint(\"Num uniq Mask values:\", np.unique(mask_tensor, return_counts=True))\n\nimage = np.rot90(montage(img_tensor))\nmask = np.rot90(montage(mask_tensor)) \n\nfig, ax = plt.subplots(1, 1, figsize = (20, 20))\nax.imshow(image, cmap ='bone')\nax.imshow(np.ma.masked_where(mask == False, mask),\n           cmap='cool', alpha=0.6)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:17.726339Z","iopub.execute_input":"2021-05-30T17:44:17.726608Z","iopub.status.idle":"2021-05-30T17:44:20.182283Z","shell.execute_reply.started":"2021-05-30T17:44:17.72658Z","shell.execute_reply":"2021-05-30T17:44:20.181493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metric and Loss","metadata":{}},{"cell_type":"code","source":"def dice_coef_metric(probabilities: torch.Tensor,\n                     truth: torch.Tensor,\n                     treshold: float = 0.5,\n                     eps: float = 1e-9) -> np.ndarray:\n    \n    scores = []\n    num = probabilities.shape[0]\n    predictions = (probabilities >= treshold).float()\n    assert(predictions.shape == truth.shape)\n    for i in range(num):\n        prediction = predictions[i]\n        truth_ = truth[i]\n        intersection = 2.0 * (truth_ * prediction).sum()\n        union = truth_.sum() + prediction.sum()\n        if truth_.sum() == 0 and prediction.sum() == 0:\n            scores.append(1.0)\n        else:\n            scores.append((intersection + eps) / union)\n    return np.mean(scores)\n\n\ndef jaccard_coef_metric(probabilities: torch.Tensor,\n               truth: torch.Tensor,\n               treshold: float = 0.5,\n               eps: float = 1e-9) -> np.ndarray:\n    \n    scores = []\n    num = probabilities.shape[0]\n    predictions = (probabilities >= treshold).float()\n    assert(predictions.shape == truth.shape)\n\n    for i in range(num):\n        prediction = predictions[i]\n        truth_ = truth[i]\n        intersection = (prediction * truth_).sum()\n        union = (prediction.sum() + truth_.sum()) - intersection + eps\n        if truth_.sum() == 0 and prediction.sum() == 0:\n            scores.append(1.0)\n        else:\n            scores.append((intersection + eps) / union)\n    return np.mean(scores)\n\n\nclass Meter:\n    '''for storing and updating iou and dice scores.'''\n    def __init__(self, treshold: float = 0.5):\n        self.threshold: float = treshold\n        self.dice_scores: list = []\n        self.iou_scores: list = []\n    \n    def update(self, logits: torch.Tensor, targets: torch.Tensor):\n        \n        probs = torch.sigmoid(logits)\n        dice = dice_coef_metric(probs, targets, self.threshold)\n        iou = jaccard_coef_metric(probs, targets, self.threshold)\n        \n        self.dice_scores.append(dice)\n        self.iou_scores.append(iou)\n    \n    def get_metrics(self) -> np.ndarray:\n        \n        dice = np.mean(self.dice_scores)\n        iou = np.mean(self.iou_scores)\n        return dice, iou\n\n\nclass DiceLoss(nn.Module):\n    \"\"\"Calculate dice loss.\"\"\"\n    def __init__(self, eps: float = 1e-9):\n        super(DiceLoss, self).__init__()\n        self.eps = eps\n        \n    def forward(self,\n                logits: torch.Tensor,\n                targets: torch.Tensor) -> torch.Tensor:\n        \n        num = targets.size(0)\n        probability = torch.sigmoid(logits)\n        probability = probability.view(num, -1)\n        targets = targets.view(num, -1)\n        assert(probability.shape == targets.shape)\n        \n        intersection = 2.0 * (probability * targets).sum()\n        union = probability.sum() + targets.sum()\n        dice_score = (intersection + self.eps) / union\n        #print(\"intersection\", intersection, union, dice_score)\n        return 1.0 - dice_score\n        \n        \nclass BCEDiceLoss(nn.Module):\n    \"\"\"Compute objective loss: BCE loss + DICE loss.\"\"\"\n    def __init__(self):\n        super(BCEDiceLoss, self).__init__()\n        self.bce = nn.BCEWithLogitsLoss()\n        self.dice = DiceLoss()\n        \n    def forward(self, \n                logits: torch.Tensor,\n                targets: torch.Tensor) -> torch.Tensor:\n        assert(logits.shape == targets.shape)\n        dice_loss = self.dice(logits, targets)\n        bce_loss = self.bce(logits, targets)\n        \n        return bce_loss + dice_loss\n    \n# helper functions for testing.  \ndef dice_coef_metric_per_classes(probabilities: np.ndarray,\n                                    truth: np.ndarray,\n                                    treshold: float = 0.5,\n                                    eps: float = 1e-9,\n                                    classes: list = ['WT', 'TC', 'ET']) -> np.ndarray:\n    \n    scores = {key: list() for key in classes}\n    num = probabilities.shape[0]\n    num_classes = probabilities.shape[1]\n    predictions = (probabilities >= treshold).astype(np.float32)\n    assert(predictions.shape == truth.shape)\n\n    for i in range(num):\n        for class_ in range(num_classes):\n            prediction = predictions[i][class_]\n            truth_ = truth[i][class_]\n            intersection = 2.0 * (truth_ * prediction).sum()\n            union = truth_.sum() + prediction.sum()\n            if truth_.sum() == 0 and prediction.sum() == 0:\n                 scores[classes[class_]].append(1.0)\n            else:\n                scores[classes[class_]].append((intersection + eps) / union)\n                \n    return scores\n\n\ndef jaccard_coef_metric_per_classes(probabilities: np.ndarray,\n               truth: np.ndarray,\n               treshold: float = 0.5,\n               eps: float = 1e-9,\n               classes: list = ['WT', 'TC', 'ET']) -> np.ndarray:\n\n    scores = {key: list() for key in classes}\n    num = probabilities.shape[0]\n    num_classes = probabilities.shape[1]\n    predictions = (probabilities >= treshold).astype(np.float32)\n    assert(predictions.shape == truth.shape)\n\n    for i in range(num):\n        for class_ in range(num_classes):\n            prediction = predictions[i][class_]\n            truth_ = truth[i][class_]\n            intersection = (prediction * truth_).sum()\n            union = (prediction.sum() + truth_.sum()) - intersection + eps\n            if truth_.sum() == 0 and prediction.sum() == 0:\n                 scores[classes[class_]].append(1.0)\n            else:\n                scores[classes[class_]].append((intersection + eps) / union)\n\n    return scores","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:20.183646Z","iopub.execute_input":"2021-05-30T17:44:20.18418Z","iopub.status.idle":"2021-05-30T17:44:20.228606Z","shell.execute_reply.started":"2021-05-30T17:44:20.184126Z","shell.execute_reply":"2021-05-30T17:44:20.227928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3DUnet","metadata":{}},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    \"\"\"(Conv3D -> GN -> ReLU) * 2\"\"\"\n    def __init__(self, in_channels, out_channels, num_groups=8):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            #nn.BatchNorm3d(out_channels),\n            nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n            nn.ReLU(inplace=True),\n\n            nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            #nn.BatchNorm3d(out_channels),\n            nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n            nn.ReLU(inplace=True)\n          )\n\n    def forward(self,x):\n        return self.double_conv(x)\n\n    \nclass Down(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.MaxPool3d(2, 2),\n            DoubleConv(in_channels, out_channels)\n        )\n    def forward(self, x):\n        return self.encoder(x)\n\n    \nclass Up(nn.Module):\n\n    def __init__(self, in_channels, out_channels, trilinear=True):\n        super().__init__()\n        \n        if trilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n        else:\n            self.up = nn.ConvTranspose3d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n            \n        self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n\n        diffZ = x2.size()[2] - x1.size()[2]\n        diffY = x2.size()[3] - x1.size()[3]\n        diffX = x2.size()[4] - x1.size()[4]\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2, diffZ // 2, diffZ - diffZ // 2])\n\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n    \nclass Out(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size = 1)\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass UNet3d(nn.Module):\n    def __init__(self, in_channels, n_classes, n_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        self.n_classes = n_classes\n        self.n_channels = n_channels\n\n        self.conv = DoubleConv(in_channels, n_channels)\n        self.enc1 = Down(n_channels, 2 * n_channels)\n        self.enc2 = Down(2 * n_channels, 4 * n_channels)\n        self.enc3 = Down(4 * n_channels, 8 * n_channels)\n        self.enc4 = Down(8 * n_channels, 8 * n_channels)\n\n        self.dec1 = Up(16 * n_channels, 4 * n_channels)\n        self.dec2 = Up(8 * n_channels, 2 * n_channels)\n        self.dec3 = Up(4 * n_channels, n_channels)\n        self.dec4 = Up(2 * n_channels, n_channels)\n        self.out = Out(n_channels, n_classes)\n\n    def forward(self, x):\n        x1 = self.conv(x)\n        x2 = self.enc1(x1)\n        x3 = self.enc2(x2)\n        x4 = self.enc3(x3)\n        x5 = self.enc4(x4)\n\n        mask = self.dec1(x5, x4)\n        mask = self.dec2(mask, x3)\n        mask = self.dec3(mask, x2)\n        mask = self.dec4(mask, x1)\n        mask = self.out(mask)\n        return mask","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:20.230394Z","iopub.execute_input":"2021-05-30T17:44:20.230936Z","iopub.status.idle":"2021-05-30T17:44:20.260687Z","shell.execute_reply.started":"2021-05-30T17:44:20.230896Z","shell.execute_reply":"2021-05-30T17:44:20.259841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trainer","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    \n    def __init__(self,\n                 net: nn.Module,\n                 dataset: torch.utils.data.Dataset,\n                 criterion: nn.Module,\n                 lr: float,\n                 accumulation_steps: int,\n                 batch_size: int,\n                 fold: int,\n                 num_epochs: int,\n                 path_to_csv: str,\n                 display_plot: bool = True,\n                ):\n\n        \"\"\"Initialization.\"\"\"\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        print(\"device:\", self.device)\n        self.display_plot = display_plot\n        self.net = net\n        self.net = self.net.to(self.device)\n        self.criterion = criterion\n        self.optimizer = Adam(self.net.parameters(), lr=lr)\n        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\",\n                                           patience=2, verbose=True)\n        self.accumulation_steps = accumulation_steps // batch_size\n        self.phases = [\"train\", \"val\"]\n        self.num_epochs = num_epochs\n\n        self.dataloaders = {\n            phase: get_dataloader(\n                dataset = dataset,\n                path_to_csv = path_to_csv,\n                phase = phase,\n                fold = fold,\n                batch_size = batch_size,\n                num_workers = 4\n            )\n            for phase in self.phases\n        }\n        self.best_loss = float(\"inf\")\n        self.losses = {phase: [] for phase in self.phases}\n        self.dice_scores = {phase: [] for phase in self.phases}\n        self.jaccard_scores = {phase: [] for phase in self.phases}\n         \n    def _compute_loss_and_outputs(self,\n                                  images: torch.Tensor,\n                                  targets: torch.Tensor):\n        images = images.to(self.device)\n        targets = targets.to(self.device)\n        logits = self.net(images)\n        loss = self.criterion(logits, targets)\n        return loss, logits\n        \n    def _do_epoch(self, epoch: int, phase: str):\n        print(f\"{phase} epoch: {epoch} | time: {time.strftime('%H:%M:%S')}\")\n\n        self.net.train() if phase == \"train\" else self.net.eval()\n        meter = Meter()\n        dataloader = self.dataloaders[phase]\n        total_batches = len(dataloader)\n        running_loss = 0.0\n        self.optimizer.zero_grad()\n        for itr, data_batch in enumerate(dataloader):\n            images, targets = data_batch['image'], data_batch['mask']\n            loss, logits = self._compute_loss_and_outputs(images, targets)\n            loss = loss / self.accumulation_steps\n            if phase == \"train\":\n                loss.backward()\n                if (itr + 1) % self.accumulation_steps == 0:\n                    self.optimizer.step()\n                    self.optimizer.zero_grad()\n            running_loss += loss.item()\n            meter.update(logits.detach().cpu(),\n                         targets.detach().cpu()\n                        )\n            \n        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n        epoch_dice, epoch_iou = meter.get_metrics()\n        \n        self.losses[phase].append(epoch_loss)\n        self.dice_scores[phase].append(epoch_dice)\n        self.jaccard_scores[phase].append(epoch_iou)\n\n        return epoch_loss\n        \n    def save_epoch_data(self, epoch):\n        filename = \"epoch_data_\"+str(epoch)+\".pth.tar\"\n        state = {\n            'epoch': epoch + 1,\n            'state_dict': self.net.state_dict(),\n            'criterion': self.criterion,\n            'optimizer' : self.optimizer.state_dict(),\n            'best_loss' : self.best_loss,\n            'losses' : self.losses,\n            'dice_scores' : self.dice_scores,\n            'jaccard_scores' : self.jaccard_scores,\n            'accumulation_steps' : self.accumulation_steps\n        }\n       # print(\"state - \",state)\n       # print(\"filename - \",filename)\n        print('Saving checkpoint in' + filename)\n        torch.save(state, filename)\n        print(\"Epoch data saved\")\n        \n    def load_checkpoint(self,filename):\n        checkpoint_data = torch.load(filename)\n        self.net.load_state_dict(checkpoint_data['state_dict'])\n        self.criterion = checkpoint_data['criterion']\n        self.optimizer.load_state_dict(checkpoint_data['optimizer'])\n        self.best_loss = checkpoint_data['best_loss']\n        self.losses = checkpoint_data['losses']\n        self.dice_score = checkpoint_data['dice_scores']\n        self.jaccard_scores = checkpoint_data['jaccard_scores']\n        self.accumulation_steps = checkpoint_data['accumulation_steps']\n        return checkpoint_data['epoch']\n        \n    \n    def run(self, is_resume = None):\n        epoch_resume = 0\n        if is_resume is not None:\n            epoch_resume = self.load_checkpoint(is_resume)\n            \n        for epoch in range(epoch_resume,self.num_epochs):\n            self._do_epoch(epoch, \"train\")\n            with torch.no_grad():\n                val_loss = self._do_epoch(epoch, \"val\")\n                self.scheduler.step(val_loss)\n            if self.display_plot:\n                self._plot_train_history()\n                \n            if val_loss < self.best_loss:\n                print(f\"\\n{'#'*20}\\nSaved new checkpoint\\n{'#'*20}\\n\")\n                self.best_loss = val_loss\n                torch.save(self.net.state_dict(), \"best_model.pth\")\n            \n            self.save_epoch_data(epoch)\n            print('saved epoch')\n            print()\n        self._save_train_history()\n            \n    def _plot_train_history(self):\n        data = [self.losses, self.dice_scores, self.jaccard_scores]\n        colors = ['deepskyblue', \"crimson\"]\n        labels = [\n            f\"\"\"\n            train loss {self.losses['train'][-1]}\n            val loss {self.losses['val'][-1]}\n            \"\"\",\n            \n            f\"\"\"\n            train dice score {self.dice_scores['train'][-1]}\n            val dice score {self.dice_scores['val'][-1]} \n            \"\"\", \n                  \n            f\"\"\"\n            train jaccard score {self.jaccard_scores['train'][-1]}\n            val jaccard score {self.jaccard_scores['val'][-1]}\n            \"\"\",\n        ]\n        \n        clear_output(True)\n        with plt.style.context(\"seaborn-dark-palette\"):\n            fig, axes = plt.subplots(3, 1, figsize=(8, 10))\n            for i, ax in enumerate(axes):\n                ax.plot(data[i]['val'], c=colors[0], label=\"val\")\n                ax.plot(data[i]['train'], c=colors[-1], label=\"train\")\n                ax.set_title(labels[i])\n                ax.legend(loc=\"upper right\")\n                \n            plt.tight_layout()\n            plt.show()\n            \n    def load_predtrain_model(self,\n                             state_path: str):\n        self.net.load_state_dict(torch.load(state_path))\n        print(\"Pretrained model loaded\")\n        \n    def _save_train_history(self):\n        \"\"\"writing model weights and training logs to files.\"\"\"\n        torch.save(self.net.state_dict(),f\"last_epoch_model.pth\")\n        \n\n        logs_ = [self.losses, self.dice_scores, self.jaccard_scores]\n        log_names_ = [\"_loss\", \"_dice\", \"_jaccard\"]\n        logs = [logs_[i][key] for i in list(range(len(logs_)))\n                         for key in logs_[i]]\n        log_names = [key+log_names_[i] \n                     for i in list(range(len(logs_))) \n                     for key in logs_[i]\n                    ]\n        #pd.DataFrame(\n        #    dict(zip(log_names, logs))\n        #).to_csv(\"train_log.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:20.263833Z","iopub.execute_input":"2021-05-30T17:44:20.264347Z","iopub.status.idle":"2021-05-30T17:44:20.428293Z","shell.execute_reply.started":"2021-05-30T17:44:20.26431Z","shell.execute_reply":"2021-05-30T17:44:20.42735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"training process","metadata":{}},{"cell_type":"code","source":"nodel = UNet3d(in_channels=4, n_classes=3, n_channels=24).to('cuda')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:20.430225Z","iopub.execute_input":"2021-05-30T17:44:20.430624Z","iopub.status.idle":"2021-05-30T17:44:20.504294Z","shell.execute_reply.started":"2021-05-30T17:44:20.430573Z","shell.execute_reply":"2021-05-30T17:44:20.503612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(net=nodel,\n                  dataset=BratsDataset,\n                  criterion=BCEDiceLoss(),\n                  lr=5e-4,\n                  accumulation_steps=4,\n                  batch_size=1,\n                  fold=0,\n                  num_epochs=1,\n                  path_to_csv = config.path_to_csv,)\n\nif config.pretrained_model_path is not None:\n    trainer.load_predtrain_model(config.pretrained_model_path)\n    \n    # if need - load the logs.      \ntrain_logs = pd.read_csv(config.train_logs_path)\ntrainer.losses[\"train\"] =  train_logs.loc[:, \"train_loss\"].to_list()\ntrainer.losses[\"val\"] =  train_logs.loc[:, \"val_loss\"].to_list()\ntrainer.dice_scores[\"train\"] = train_logs.loc[:, \"train_dice\"].to_list()\ntrainer.dice_scores[\"val\"] = train_logs.loc[:, \"val_dice\"].to_list()\ntrainer.jaccard_scores[\"train\"] = train_logs.loc[:, \"train_jaccard\"].to_list()\ntrainer.jaccard_scores[\"val\"] = train_logs.loc[:, \"val_jaccard\"].to_list()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:20.50729Z","iopub.execute_input":"2021-05-30T17:44:20.507532Z","iopub.status.idle":"2021-05-30T17:44:21.082525Z","shell.execute_reply.started":"2021-05-30T17:44:20.507508Z","shell.execute_reply":"2021-05-30T17:44:21.081773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainer_run():\n    trainer.run()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:21.085515Z","iopub.execute_input":"2021-05-30T17:44:21.085765Z","iopub.status.idle":"2021-05-30T17:44:21.091752Z","shell.execute_reply.started":"2021-05-30T17:44:21.085739Z","shell.execute_reply":"2021-05-30T17:44:21.090958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiments and Results","metadata":{}},{"cell_type":"code","source":"def compute_scores_per_classes(model,\n                               dataloader,\n                               classes):\n    \n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    dice_scores_per_classes = {key: list() for key in classes}\n    iou_scores_per_classes = {key: list() for key in classes}\n\n    with torch.no_grad():\n        for i, data in enumerate(dataloader):\n            imgs, targets = data['image'], data['mask']\n            imgs, targets = imgs.to(device), targets.to(device)\n            logits = model(imgs)\n            logits = logits.detach().cpu().numpy()\n            targets = targets.detach().cpu().numpy()\n            \n            dice_scores = dice_coef_metric_per_classes(logits, targets)\n            iou_scores = jaccard_coef_metric_per_classes(logits, targets)\n\n            for key in dice_scores.keys():\n                dice_scores_per_classes[key].extend(dice_scores[key])\n\n            for key in iou_scores.keys():\n                iou_scores_per_classes[key].extend(iou_scores[key])\n\n    return dice_scores_per_classes, iou_scores_per_classes\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:21.095181Z","iopub.execute_input":"2021-05-30T17:44:21.095444Z","iopub.status.idle":"2021-05-30T17:44:21.107431Z","shell.execute_reply.started":"2021-05-30T17:44:21.095418Z","shell.execute_reply":"2021-05-30T17:44:21.106269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_dataloader = get_dataloader(BratsDataset, 'train_data.csv', phase='valid', fold=0)\nlen(dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:21.108806Z","iopub.execute_input":"2021-05-30T17:44:21.109148Z","iopub.status.idle":"2021-05-30T17:44:21.125288Z","shell.execute_reply.started":"2021-05-30T17:44:21.10911Z","shell.execute_reply":"2021-05-30T17:44:21.124244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nodel.eval();","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:21.126836Z","iopub.execute_input":"2021-05-30T17:44:21.127234Z","iopub.status.idle":"2021-05-30T17:44:21.13253Z","shell.execute_reply.started":"2021-05-30T17:44:21.127199Z","shell.execute_reply":"2021-05-30T17:44:21.131534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"%%time\ndice_scores_per_classes, iou_scores_per_classes = compute_scores_per_classes(\n    nodel, val_dataloader, ['WT', 'TC', 'ET']\n    )\n\ndice_df = pd.DataFrame(dice_scores_per_classes)\ndice_df.columns = ['WT dice', 'TC dice', 'ET dice']\n\niou_df = pd.DataFrame(iou_scores_per_classes)\niou_df.columns = ['WT jaccard', 'TC jaccard', 'ET jaccard']\nval_metics_df = pd.concat([dice_df, iou_df], axis=1, sort=True)\nval_metics_df = val_metics_df.loc[:, ['WT dice', 'WT jaccard', \n                                      'TC dice', 'TC jaccard', \n                                      'ET dice', 'ET jaccard']]\nval_metics_df.sample(5)\n\ncolors = ['#35FCFF', '#FF355A', '#96C503', '#C5035B', '#28B463', '#35FFAF']\npalette = sns.color_palette(colors, 6)\n\nfig, ax = plt.subplots(figsize=(12, 6));\nsns.barplot(x=val_metics_df.mean().index, y=val_metics_df.mean(), palette=palette, ax=ax);\nax.set_xticklabels(val_metics_df.columns, fontsize=14, rotation=15);\nax.set_title(\"Dice and Jaccard Coefficients from Validation\", fontsize=20)\n\nfor idx, p in enumerate(ax.patches):\n        percentage = '{:.1f}%'.format(100 * val_metics_df.mean().values[idx])\n        x = p.get_x() + p.get_width() / 2 - 0.15\n        y = p.get_y() + p.get_height()\n        ax.annotate(percentage, (x, y), fontsize=15, fontweight=\"bold\")\n\nfig.savefig(\"result1.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\nfig.savefig(\"result1.svg\", format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"## More visualization","metadata":{}},{"cell_type":"code","source":"def compute_results(model,\n                    dataloader,\n                    treshold=0.33):\n\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    resultsT = {\"Id\": [],\"image\": [], \"GT\": [],\"Prediction\": []}\n\n    with torch.no_grad():\n        for i, data in enumerate(dataloader):\n            id_, imgs, targets = data['Id'], data['image'], data['mask']\n            imgs, targets = imgs.to(device), targets.to(device)\n            logits = model(imgs)\n            probs = torch.sigmoid(logits)\n            \n            predictions = (probs >= treshold).float()\n            predictions =  predictions.cpu()\n            targets = targets.cpu()\n            \n            resultsT[\"Id\"].append(id_)\n            resultsT[\"image\"].append(imgs.cpu())\n            resultsT[\"GT\"].append(targets)\n            resultsT[\"Prediction\"].append(predictions)\n            \n            # only 5 pars\n            if (i > 5):    \n                return resultsT\n        return resultsT","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:23.469402Z","iopub.execute_input":"2021-05-30T17:44:23.469716Z","iopub.status.idle":"2021-05-30T17:44:23.479549Z","shell.execute_reply.started":"2021-05-30T17:44:23.469686Z","shell.execute_reply":"2021-05-30T17:44:23.478714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dataload_pred():\n    resultsT = compute_results(nodel, val_dataloader, 0.33)\n    for id_, img, gt, prediction in zip(resultsT['Id'][4:],\n                                        resultsT['image'][4:],\n                                        resultsT['GT'][4:],\n                                        resultsT['Prediction'][4:]):\n        print(id_)\n        break\n        \n    print(\"after break\")\n    show_result = ShowResult()\n    show_result.plot(img, gt, prediction)\n    print(\"after show_result\")\n    prediction = prediction.squeeze().cpu().detach().numpy()\n    prediction = np.moveaxis(prediction, (0, 1, 2, 3), (0, 3, 2, 1))\n    wt,tc,et = prediction\n    print(wt.shape, tc.shape, et.shape)\n    prediction = (wt + tc + et)\n    prediction = np.clip(prediction, 0, 1)\n    print(prediction.shape)\n    print(\"after predition transform\")\n    title = \"Prediction\"\n    filename2 = \"prediction.gif\"\n    \n    data_to_3dgif = Image3dToGIF3d(img_dim = (120, 120, 78), binary=True, normalizing=False)\n    transformed_data = data_to_3dgif.get_transformed_data(prediction)\n    data_to_3dgif.plot_cube(\n        transformed_data,\n        title=title,\n        make_gif=True,\n        path_to_save=filename2\n    )\n    show_gif(filename2, format='png')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:25.030338Z","iopub.execute_input":"2021-05-30T17:44:25.030652Z","iopub.status.idle":"2021-05-30T17:44:25.042145Z","shell.execute_reply.started":"2021-05-30T17:44:25.030625Z","shell.execute_reply":"2021-05-30T17:44:25.041309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3d binary mask projection for ground truth and prediction","metadata":{}},{"cell_type":"markdown","source":"ground truth","metadata":{}},{"cell_type":"markdown","source":"prediction","metadata":{}},{"cell_type":"code","source":"def pred_loader():\n    convert_3d_2d()\n    prediction = prediction.squeeze().cpu().detach().numpy()\n    prediction = np.moveaxis(prediction, (0, 1, 2, 3), (0, 3, 2, 1))\n    wt,tc,et = prediction\n    print(wt.shape, tc.shape, et.shape)\n    prediction = (wt + tc + et)\n    prediction = np.clip(prediction, 0, 1)\n    print(prediction.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gen_pred():\n    pred_loader()\n    \n    title = \"Prediction\"\n    filename2 = \"prediction.gif\"\n    \n    data_to_3dgif = Image3dToGIF3d(img_dim = (120, 120, 78), binary=True, normalizing=False)\n    transformed_data = data_to_3dgif.get_transformed_data(prediction)\n    data_to_3dgif.plot_cube(\n        transformed_data,\n        title=title,\n        make_gif=True,\n        path_to_save=filename2\n    )\n    show_gif(filename2, format='png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"result","metadata":{}},{"cell_type":"code","source":"from distutils.dir_util import copy_tree\nfromdir = \"../input/flaskapp/srm (2)/srm\"\ntodir = \".\"\ncopy_tree(fromdir,todir)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:32.263299Z","iopub.execute_input":"2021-05-30T17:44:32.26361Z","iopub.status.idle":"2021-05-30T17:44:32.392382Z","shell.execute_reply.started":"2021-05-30T17:44:32.263582Z","shell.execute_reply":"2021-05-30T17:44:32.391709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loader():\n    trainer_run()\n    dataload_pred()\n    import shutil\n    shutil.copyfile('./prediction.gif', \"./static/assets/img/prediction.gif\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:36.623956Z","iopub.execute_input":"2021-05-30T17:44:36.624286Z","iopub.status.idle":"2021-05-30T17:44:36.630702Z","shell.execute_reply.started":"2021-05-30T17:44:36.624256Z","shell.execute_reply":"2021-05-30T17:44:36.62876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install flask-ngrok\nfrom flask import Flask, render_template, request, url_for, session\nimport os\nfrom werkzeug.utils import secure_filename, redirect\nfrom distutils.dir_util import copy_tree\nimport time\n\nfrom flask_ngrok import run_with_ngrok\n\n\napp = Flask(__name__)\nrun_with_ngrok(app)\n\napp.secret_key = \"hey\"\n\n@app.route(\"/\")\ndef home():\n    return render_template(\"index.html\")\n\n@app.route(\"/about\")\ndef about():\n    return render_template(\"about.html\")\n\n@app.route(\"/display\")\ndef display():\n    return render_template(\"post.html\")\n\n@app.route(\"/uploader\", methods=[\"GET\",\"POST\"])\ndef uploader():\n    if(request.method==\"POST\"):\n        f = request.files[\"file1\"]\n        f.save(os.path.join(\"static\",secure_filename(f.filename)))\n        loader()\n        \n    return redirect(url_for(\"display\"))\n    \n@app.route(\"/download_file\")\ndef download_file():\n    path=\"./static/assets/img/prediction.gif\"\n    return send_file(path,as_attachment=True)\n\napp.run()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T17:44:37.772776Z","iopub.execute_input":"2021-05-30T17:44:37.773092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}